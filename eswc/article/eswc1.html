<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" 
  prefix="
  schema: http://schema.org/
  prism: http://prismstandard.org/namespaces/basic/2.0/
  dcterms: http://purl.org/dc/terms/">
  <head>
    <!-- Visualisation requirements (mandatory for optimal reading) -->
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="stylesheet" href="../css/bootstrap.min.css"/>
    <link rel="stylesheet" href="../css/rash.css"/>
    <script src="../js/jquery.min.js"><![CDATA[ ]]></script>
    <script src="../js/bootstrap.min.js"><![CDATA[ ]]></script>
    <script src="../js/rash.js"><![CDATA[ ]]></script>
    <!-- /END Visualisation requirements (mandatory for optimal reading) -->

    <title property="dcterms:title">AMI-Table: Creation of Semantic Tables from PDF documents</title>

    <!-- Author's data (one or more) -->
    <meta about="#pmr" property="schema:name" name="dc.creator" content="Peter Murray-Rust"/>
    <meta about="#pmr" property="schema:email" content="peter@contentmine.org"/>
    <link about="#pmr" property="schema:affiliation" href="#contentmine"/>
    <link about="#pmr" property="schema:affiliation" href="#chem-unicam"/>

    <meta about="#chjh" property="schema:name" name="dc.creator" content="Chris H J Hartgerink"/>
    <meta about="#chjh" property="schema:email" content="chjh@protonmail.com"/>
    <link about="#chjh" property="schema:affiliation" href="#contentmine"/>
    <link about="#chjh" property="schema:affiliation" href="#uni-tilburg"/>

    <!-- Affiliations -->
    <meta about="#contentmine" property="schema:name" content="ContentMine Ltd, Future Business Centre, Kings Hedges Road, CB4 2HY, Cambridge, UK"/>
    <meta about="#chem-unicam" property="schema:name" content="Department of Chemistry, University of Cambridge, CB2 1EW, UK"/>
    <meta about="#uni-tilburg" property="schema:name" content="Methodology and Statistics Tilburg School of Social and Behavioral Sciences Tilburg University"/>

    <!-- Paper keywords (one or more) -->
    <meta property="prism:keyword" content="PDF"/>
    <meta property="prism:keyword" content="semantic web"/>
	
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML"> </script>
	<style>
	.monospace {font-family : monospace;}
	.border {border:solid 1px;}
	section {border: solid red 1px; margin: 1px;}
	</style>
	
  </head>
  
  <body>
	  
    <section id="SectionAbstract" role="doc-abstract">
      <h1>Abstract</h1>
      <p>Tables in PDF documents are located (<em>retrieved</em>) and the contents are semantically <em>extracted</em> into valid XHTML tables. 
		  These tables can transformed into <em>Tidy Data</em> which is then searchable using XPath and SPARQL. The workflow uses 
		  Apache <em>PDFBox</em> to create a stream of characters and graphics paths in normalized page coordinates. The tables are located with 
		  <em>GROBID</em> which passes coordinates to <em>AMI</em> to extract contents (caption, and contents). AMI futher analyses contents
		  into title, header, body, footer. The header and body are further extracted into "rows" and "columns". Since many "tables" are not a strict 
		  cartesian product of rows and columns, we develop the semantics of "split-column", "subheaders", "subtables" and "split-row". 
		  These can be normalised into <em>HTML5</em> and <em>Tidy data</em> allowing search by semantic tools.
	  </p>
	  <p>We developed the system against 20 journals in the biomedical sciences giving 50 articles and 170 tables. Most were created in APA-like style 
		  (American Psychological Association) style, with multi-row split-column headers and frequent subtables and few explict table cells.  
		  We classified tables as messy, untidy and tidy (usually single header rows and no subtables). GROBID ws used to extract table coordinates 
		  and gave metrics of 91 TP, 20 FP, and 21 FN. AMI was tested separately by manually excising 167 true tables. These were transformed into raw SVG
		  files, and the contents re-assembled heuristically into high-level primitives (words, phrases, styles, superscripts) and regions (lines, boxes).
		  Heuristics were used to create table rows, columns and cells which were populated with HTML content. 
	  </p>
	  <p>
		 The analysis was blinded from the developers. We used expert classification of the raw table structure into a matrix of (contents {bad, reasonable, close to perfect, perfect}). AMI was used to extract HTML5 from these with the same expert classification. Of 106 tables, 33 were perfect:perfect (33%) with 70% (reasonable:reasonable) or better. 
      </p>
	  <p> The project is ongoing and carried out as Open Notebook Science where all data and discussion is immediately uploaded to the public web. This paper is in part an overlay on the Open Notebook and material is linked rather than being copied (with semantic loss and error). All data and code is on or referenced from http://github.com/contentmine/cm-ucl. Discussion, including summary results is at http://discuss.contentmine.org/t/ami-eppi-cm-ucl-table-extraction-project/322 which has many sequential postings commenting on strategy and progress.</p>
    </section>
	
    <section>
      <h1>Introduction</h1>
	  <section class="subsection">
		  <h2>Note on Strategy</h2>
		  <em>
	  <p>
	  	This paper reports on progress in extracting semantic data from tables in PDF documents, especially in the biomedical sciences. We have written a general, heuristic approach which covers a range of tables and which we believe will perform well against the gridded tables in the ESWC2017 challenge. If this paper is accepted we will tune the tools for EWSC-like tables (mainly explicitly gridded) and submit a results paper before the Meeting. 
	  </p>
	  <p>All our work is carried out as Open Notebook Science (Bradley) where all parts of the process are visible to the world as it happens (e.g. daily or better commits). We also believe in Collaborative Science, exemplified by the Human Genome project, GalaxyZoo, and OpenSourceMalaria. We will announce this project to the world and invite collaboration, with the aim to refining the heuristics (and, where appropriate, Machine Learning).
	  </p>
  </em>
     </section>

	  <section class="subsection">
		 <h2>What is a Table?</h2>
		 To extract tables we need a target schema, but few of these have a semantic structure. 
	  <section class="subsubsection">
				 <h3>Simple table</h3>
				 The simplest tables, as exemplified by the CSV (TSV) format, consist of the cartesian product of <em>rows</em> and <em>columns</em>, ideally with row and column labels.
			 <table>
				 <caption>simple cartesian table. </caption>
				 <tr>
					 <td>
			 <table>
				 <tr>
					 <th class="sample">Sample</th>
					 <th class="age_yr">Age(yr)</th>
					 <th class="mass_y">mass(g)</th>
				 </tr>
				 <tr>
					 <td class="rowtitle">A123</td>
					 <td>23</td>
					 <td>99</td>
				 </tr>
				 <tr>
					 <td class="rowtitle">A456</td>
					 <td>33</td>
					 <td>56</td>
				 </tr>
				 <tr>
					 <td class="rowtitle">A678</td>
					 <td>34</td>
					 <td>57</td>
				 </tr>
					</table>
				</td>
				<td class="monospace">
	   			 &lt;table><br/>
	   				 &lt;tr>
	   					 &lt;th>Sample&lt;/th>
	   					 &lt;th>Age(yr)&lt;/th>
	   					 &lt;th>mass(g)&lt;/th>
	   				 &lt;/tr><br/>
	   				 &lt;tr>
	   					 &lt;td>A123&lt;/td>
	   					 &lt;td>23&lt;/td>
	   					 &lt;td>99&lt;/td>
	   				 &lt;/tr><br/>
	   				 &lt;tr>
	   					 &lt;td>A456&lt;/td>
	   					 &lt;td>33&lt;/td>
	   					 &lt;td>56&lt;/td>
	   				 &lt;/tr><br/>
	   				 &lt;tr>
	   					 &lt;td>A678&lt;/td>
	   					 &lt;td>34&lt;/td>
	   					 &lt;td>57&lt;/td>
	   				 &lt;/tr><br/>
				&lt;/table>
			</td>
</tr>
		</table>
		The task is to go from the left side (with no semantic structure) to the right, using heuristics.
		Here is a simple table in the wild which AMI has converted perfectly to semantic HTML:
		<img src="_perfect1.png" class="border"/><br/>
		(Note: It is marred by the typesetter's use of an undocumented font ("AdvPSMP13") where the symbol for &mu; is represented by "1".)
		
	</section>
	
	  <section class="subsubsection">
		<h3>Complex table</h3>
		Many scientific tables have more complex semantics, often implicit and only indicated by visual clues. 
		<figure>
				<table height="65%">
					<tr><th>Original (screen shot)</th>
						<th>AMI-annotation</th>
					</tr>
					<tr>
				
						<td><img height="75%" width="75%" src="_subtable_a.png"/></td>
						<td><img src="_subtables.png"/></td>
		</tr>
			</table>
</figure>		
		
		Common implicit semantics are:
		<ul>
		<li><b>split columns</b>. Here the column semantics are tree-structured, with high-level column-names being split into tuples at each level. 
		This example shows a single split (Velazodone), but we have found up to 4 levels, with irregularity such as:
		<figure>
		<img class="border" src="complexHeader.png"/>
	</figure>
</li>
	<li><b>subtables</b>. Here there are 3 subtables with varying rows: (5-HT<sub>1A</sub> (4 rows), 5-HT<sub>2A</sub> (2 rows), 5-HT<sub>T</sub> (8 rows)). </li>
</ul>
	</section>
			
	 </section>
	 
	  <section class="subsection">
		 <h2>PDF Documents</h2>
		 <p>
			 PDF documents have very little structure; simply character streams, graphicsPaths, and bitmaps, but no support for higher primitives such as words, phrases, lines, rectangles, etc. Table reconstruction therefore requires heuristics. Our process includes:
			 <ul>
				 <li><b>normalization of character stream</b>. Unfortunately many publishers use non-standard, undocumented, font-families. While the US-ASCII range(32-127) is normally correct, higher code points, especially maths, are frequently totally corrupted. Other problems include diacritics, ligatures, small caps, etc. TeX-based fonts and Mathematical PI  can suffer from this. Many publishers (typesetters) have arcane fonts ("Adv"). AMI has conversion tables for the commonest. See Figure </li>
					 <li><b>Sub and superscripts (figure 1)</b>. Sub- and super-scripts ("suscripts") are critical for STEM subjects but PDF can only represent them by different-sized characters offset in X and Y. Suscripts cause large problems in adding semantics to PDFs; in AMI we convert them to semantic form at an early stage and preserve them through the process.</li>
					 <li><b>Indents</b>. Authors (or typesetters) will indent cells to indicate (a) continuation os running text (b) subtables. There is no standard and much error.</li>
					 <li><b>Subtables</b>. Authors may break the body of the table into sub-sections. Sometimes these are completely new tables, sometimes they are subcategories of preceding information. Subtables are almost always implicit and may by indicated by (a) interleaved (bold) title rows (b) different backgrounds (c) nested boxes, etc. Again there are no explicit standards
					 </li>
					 <li><b>Scientific illiteracy</b>. It is appallingly common that typsetters retype minus signs ("-") as em-dashes (U+2014, &mdash;) as in Figure . or create not-equals (U+2260, &#x2260;) by overstriking "=" with "/". We are slowly developing heuristics to detect and correct these.</li>
				 </ul>
		 </p>
	 </section>
 </section>
 
	  <section class="section">
        <h1>Results</h1>
		The tasks can be split into the following:
		<ul>
			<li><b>Conversion of PDFs to character streams</b>. Two complementary approaches were used:
				<ul>
					<li><b>Grobid</b>. Grobid (originally created to extract bibliographic information) reads a PDF document and outputs XML according to the TEI schema (<tt>xmlns="http://www.tei-c.org/ns/1.0"</tt>); a typical result is <a href="https://github.com/ContentMine/cm-ucl/blob/master/corpus-oa-pmr-v01/10.1007_s00213-015-4198-1/fulltext.grobid.xml">grobid.xml[1]</a>. Grobid uses PDFBox 1.8 as the orimary parser and uses its output to create chunks of text, e.g.
						<pre>
&lt;figDesc 
coords="7,306.14,57.96,238.05,7.71;7,306.14,67.88,237.58,7.71">
Table 1 Mean percent changes from vehicle in serotonergic markers after chronic (14 days)
 vilazodone, citalopram, and paroxetine treatment&lt;/figDesc>
&lt;table coords="7,306.14,86.76,238.14,314.59">Brain region Vilazodone (%) 
Citalopram (%) Paroxetine (%)
						</pre>
						Grobid uses machine-learning (CRF) to segment the document and classify components and annotate with page coordinates. 
					</li>
					<li>
						Our AMI suite also uses PDFBox to create XML - first SVG and then HTML.
						<table>
							<p><em>The dotted lines indicate how the development and validation corpus was created.</em></p>
							<tr>
								<td width="38%"><img src="_architecture.png"/></td>
								<td><p>The PDF is parsed independently to <tt>grobid.xml</tt> and also to a stream (<tt>pdf.svg</tt> of characters 
									(<tt>svg:text</tt>) and graphics paths (<tt>svg:path</tt>). The character stream is processed for sub- and super-scripts and combined into words->phrases->sentences. The paths are heuristically transformed to higher semantic primitives (mainly <tt>svg:rect</tt> and  <tt>svg:line</tt>)</td>
							</tr>
						</table>
					</li>
				</ul>
					This was carried out by our AMI suite of tools (https://github.com/ContentMine/ami/ and ) </li>
		</ul>
		<section class="subsection">
			<h2>Retrieval of tables</h2>
			
				We used the GROBID program to extract the main structure and text of the documents. The report is at: https://github.com/ContentMine/cm-ucl/interim-reports/grobid-table-retrieval.pdf
		<table>GROBID
			<tr>
				<th/>
				<th>No table</th>
				<th>Table</th>
				<th>Total</th>
			</tr>
			<tr>
				<td>"No table"</td>
				<td>-</td>
				<td>24 (0.209)</td>
				<td>24 (0.209)</td>
			</tr>
			<tr>
				<td>"Table"</td>
				<td>21 (0.183)</td>
				<td>70 (0.609)</td>
				<td>91 (0.791)</td>
			</tr>
			<tr>
				<td>Total</td>
				<td>21 (0.183)</td>
				<td>94 (0.817)</td>
				<td>115 (1)</td>
			</tr>
					
		</table>
		

		The precision of using GROBID to extract tables was 0.769 whereas the recall was 0.745. The table above
		depicts the classification problem, with columns indicating the true situation and the rows indicating the
		result from GROBID. There were 21 false positives and 70 true positives, resulting in a estimated Positive
		Predictive Value (PPV) of 0.769.
		
      </section>
	  
		    <section class="subsection">
		  <h1>Table structure</h1>
		 <table>
			 <tr>
				 <th><em>AMI \ original</em></th>
				 <th>Bad</th>
				 <th>Reasonable</th>
				 <th>Excellnt</th>
				 <th>Perfect</th>
			 </tr>
			 <tr>
				 <td>Bad</td>
				 <td>21</td>
				 <td>2</td>
				 <td>1</td>
				 <td>3</td>
			 </tr>
			 <tr>
				 <td>Reasonable</td>
				 <td>3</td>
				 <td>8</td>
				 <td>0</td>
				 <td>3</td>
			 </tr>
			 <tr>
				 <td>Excellent</td>
				 <td>1</td>
				 <td>2</td>
				 <td>7</td>
				 <td>21</td>
			 </tr>
			 <tr>
				 <td>Perfect</td>
				 <td>0</td>
				 <td>0</td>
				 <td>1</td>
				 <td>33</td>
			 </tr>
				</table>
	  </section>

    <section id="SectionAcknowledgements" role="doc-acknowledgements">
      <h1>Acknowledgements</h1>
      <ul>
		  <li>To Prof. James Thomas and Alison O'Mara-Hayes at University College , London for funding and in-depeth discussions.</li>
		  <li>To Laurent Romary and Patrice XXX for the Grobid.</li>
	  </ul>
    </section>

GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications. P. Lopez. Proceedings of the 13th European Conference on Digital Library (ECDL), Corfu, Greece, 2009.

Automatic Extraction and Resolution of Bibliographical References in Patent Documents. P. Lopez. First Information Retrieval Facility Conference (IRFC), Vienna, May 2010. LNCS 6107, pp. 120-135. Springer, Heidelberg (2010).

Evaluation of Header Metadata Extraction Approaches and 
Tools
 for Scientific PDF Documents 
Mario Lipinski, Kevin Yao, Corinna Breitinger, Joeran Beel
, Bela Gipp 
    <section id="SectionReferences" role="doc-bibliography">
      <h1>References</h1>
      <ul>
        <li id="PeroniCiTOFaBiO" role="doc-biblioentry">
          <p>...</p>
        </li>

      </ul>
    </section>

[1] https://github.com/ContentMine/cm-ucl/blob/master/corpus-oa-pmr-v01/10.1007_s00213-015-4198-1/fulltext.grobid.xml
	
  </body>
</html>